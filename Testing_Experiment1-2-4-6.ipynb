{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10747,
     "status": "ok",
     "timestamp": 1586755617734,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "mfEY1THBXhap",
    "outputId": "a6a95295-e0cc-484e-e8d4-80d62078a459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
      "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
      "Gen RAM Free: 26.3 GB  | Proc size: 159.9 MB\n",
      "GPU RAM Free: 16280MB | Used: 0MB | Util   0% | Total 16280MB\n"
     ]
    }
   ],
   "source": [
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!pip install gputil\n",
    "!pip install psutil\n",
    "!pip install humanize\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "gpu = GPUs[0]\n",
    "def printm():\n",
    " process = psutil.Process(os.getpid())\n",
    " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "printm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13792,
     "status": "ok",
     "timestamp": 1586755620792,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "liUecVuaXkgZ",
    "outputId": "4fce442c-e6e1-4652-e2a5-9adca6f62f14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-gpu in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.4.1)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.9.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.1.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.18.2)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.10.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.28.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (3.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (0.8.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.12.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (1.0.8)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu) (2.1.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow-gpu) (46.1.3)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu) (2.10.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.2.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.7.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.21.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.3.0)\n",
      "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.1)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (1.24.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow-gpu) (0.4.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_LY1NAEY1ru"
   },
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16060,
     "status": "ok",
     "timestamp": 1586755623071,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "HcHw4Om8XvLm",
    "outputId": "3833cb07-5034-4b09-8ef7-f034c50e4840"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "class utility:\n",
    "\n",
    "    def read_CSV(self, filename):\n",
    "        df = pd.read_csv(filename)\n",
    "        return df\n",
    "\n",
    "    def get_text_label(self, df):\n",
    "        texts = []  # list of text samples\n",
    "        labels = []  # list of label ids\n",
    "        for index, row in df.iterrows():\n",
    "            if isinstance(row['text_cleaned'], float):\n",
    "                texts.append(str(row['text_cleaned']))\n",
    "            else:\n",
    "                texts.append(row['text_cleaned'])\n",
    "\n",
    "            labels.append(row['target'])\n",
    "\n",
    "        return texts, labels\n",
    "\n",
    "    def tokenize_texts(self, texts):\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=50000)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def padding_texts(self, texts, maxlen):\n",
    "\n",
    "        texts = tf.keras.preprocessing.sequence.pad_sequences(texts, padding='post', maxlen=maxlen)\n",
    "\n",
    "        return texts\n",
    "\n",
    "    def get_testing_metric(self, y_test, y_pred):\n",
    "        accuracyScore = accuracy_score(y_test, y_pred)\n",
    "        precisionScore = precision_score(y_test, y_pred)\n",
    "        recallScore = recall_score(y_test, y_pred)\n",
    "        f1Score = f1_score(y_test, y_pred)\n",
    "\n",
    "        return accuracyScore, precisionScore, recallScore, f1Score\n",
    "\n",
    "    def write_df_csv(self, df, out_path):\n",
    "        df.to_csv(out_path, index=False)\n",
    "\n",
    "    def create_embedding_matrix(self, filepath, word_index, embedding_dim):\n",
    "        vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "        with open(filepath, encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                word, *vector = line.split()\n",
    "                if word in word_index:\n",
    "                    idx = word_index[word]\n",
    "                    embedding_matrix[idx] = np.array(\n",
    "                        vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "        return embedding_matrix\n",
    "\n",
    "    def get_max_length_of_sentences(self, texts):\n",
    "        maxlength = 0\n",
    "        for text in texts:\n",
    "            if (len(text.split()) > maxlength):\n",
    "                maxlength = len(text.split())\n",
    "\n",
    "        return maxlength\n",
    "\n",
    "    def get_training_val_data(self, textsTraining, labelsTraining, train_index, val_index, glovePath):\n",
    "        textsTraining, textsTesting = np.asarray(textsTraining)[train_index], np.asarray(textsTraining)[val_index]\n",
    "        y_train, y_val = np.asarray(labelsTraining)[train_index], np.asarray(labelsTraining)[val_index]\n",
    "\n",
    "        # Tokenize words\n",
    "        tokenizer = self.tokenize_texts(textsTraining)\n",
    "        X_train = tokenizer.texts_to_sequences(textsTraining)\n",
    "        X_val = tokenizer.texts_to_sequences(textsTesting)\n",
    "\n",
    "        # Adding 1 because of reserved 0 index\n",
    "        vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "        # get maxlen\n",
    "        maxlen = self.get_max_length_of_sentences(textsTraining)\n",
    "\n",
    "        # Pad sequences with zeros\n",
    "        X_train = self.padding_texts(X_train, maxlen)\n",
    "        X_val = self.padding_texts(X_val, maxlen)\n",
    "\n",
    "        embedding_matrix = []\n",
    "        embedding_matrix.append(self.create_embedding_matrix(glovePath[0], tokenizer.word_index, 50))\n",
    "        embedding_matrix.append(self.create_embedding_matrix(glovePath[1], tokenizer.word_index, 100))\n",
    "        embedding_matrix.append(self.create_embedding_matrix(glovePath[2], tokenizer.word_index, 200))\n",
    "        embedding_matrix.append(self.create_embedding_matrix(glovePath[3], tokenizer.word_index, 300))\n",
    "\n",
    "        return X_train, X_val, y_train, y_val, vocab_size, maxlen, embedding_matrix\n",
    "\n",
    "    def Average(self, list):\n",
    "        return sum(list) / len(list)\n",
    "\n",
    "    def recall(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(self, y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    def f1_score(self, y_true, y_pred):\n",
    "        precision = self.precision(y_true, y_pred)\n",
    "        recall = self.recall(y_true, y_pred)\n",
    "        return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jMvyO9qEYx91"
   },
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USzyKDK0XT6w"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CNN:\n",
    "\n",
    "    def cnn_model(self, vocab_size, maxlen, embedding_matrix, indiv, path):\n",
    "        model = tf.keras.models.Sequential()\n",
    "        conv_idx = dense_idx = dropout_idx = maxpooling_idx = 0\n",
    "        for layer in path:\n",
    "            if layer == 'embedding_layer':\n",
    "                model.add(\n",
    "                    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=indiv['output_dim'],\n",
    "                                     weights=[embedding_matrix], input_length=maxlen, trainable=True))\n",
    "            elif layer == 'convolutional_layer':\n",
    "                conv_idx += 1\n",
    "                model.add(tf.keras.layers.Conv1D(indiv['num_filters'], indiv['kernel_size'],\n",
    "                                        kernel_initializer=indiv['conv_init_mode'],\n",
    "                                        activation=indiv['conv_activation_func'],\n",
    "                                        kernel_constraint=tf.keras.constraints.max_norm(indiv['conv_weight_constraint']),\n",
    "                                        data_format='channels_first'))\n",
    "            elif layer == 'dense_layer':\n",
    "                dense_idx += 1\n",
    "                model.add(tf.keras.layers.Dense(indiv['neurons'],\n",
    "                                       kernel_initializer=indiv['dense_init_mode'],\n",
    "                                       activation=indiv['dense_activation_func'],\n",
    "                                       kernel_constraint=tf.keras.constraints.max_norm(indiv['dense_weight_constraint'])))\n",
    "            elif layer == 'dropout_layer':\n",
    "                dropout_idx += 1\n",
    "                model.add(tf.keras.layers.Dropout(indiv['dropout_rate']))\n",
    "            elif layer == 'maxpooling_layer':\n",
    "                maxpooling_idx += 1\n",
    "                model.add(tf.keras.layers.MaxPooling1D(indiv['pool_size']))\n",
    "            elif layer == 'global_maxpooling_layer':\n",
    "                model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "            elif layer == 'output_layer':\n",
    "                model.add(tf.keras.layers.Dense(1, kernel_initializer=indiv['output_init_mode'], activation='sigmoid'))\n",
    "\n",
    "        if indiv['optimizer'] == 'sgd':\n",
    "            opt = tf.keras.optimizers.SGD(lr=indiv['learning_rate'], momentum=indiv['momentum'], decay=0.0,\n",
    "                                 nesterov=False)\n",
    "        if indiv['optimizer'] == 'rmsprop':\n",
    "            opt = tf.keras.optimizers.RMSprop(lr=indiv['learning_rate'], rho=0.9, epsilon=None, decay=0.0)\n",
    "        elif indiv['optimizer'] == 'adagrad':\n",
    "            opt = tf.keras.optimizers.Adagrad(lr=indiv['learning_rate'], epsilon=None, decay=0.0)\n",
    "        elif indiv['optimizer'] == 'adadelta':\n",
    "            opt = tf.keras.optimizers.Adadelta(lr=indiv['learning_rate'], rho=0.95, epsilon=None, decay=0.0)\n",
    "        elif indiv['optimizer'] == 'adam':\n",
    "            opt = tf.keras.optimizers.Adam(lr=indiv['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=None,\n",
    "                                  decay=0.0, amsgrad=False)\n",
    "        elif indiv['optimizer'] == 'adamax':\n",
    "            opt = tf.keras.optimizers.Adamax(lr=indiv['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=None,\n",
    "                                    decay=0.0)\n",
    "        elif indiv['optimizer'] == 'nadam':\n",
    "            opt = tf.keras.optimizers.Nadam(lr=indiv['learning_rate'], beta_1=0.9, beta_2=0.999, epsilon=None,\n",
    "                                   schedule_decay=0.004)\n",
    "\n",
    "        util = utility()\n",
    "        model.compile(optimizer=opt, loss='binary_crossentropy', metrics=[util.f1_score])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gal0h7YmYvNp"
   },
   "source": [
    "# Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16047,
     "status": "ok",
     "timestamp": 1586755623075,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "_VjbiI2WX6UO",
    "outputId": "6a27273e-05ce-47f5-ca61-e1493467e471"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "# path\n",
    "dataTraining = 'trainPreprocessed.csv'\n",
    "dataTesting = 'testPreprocessed.csv'\n",
    "testing_name = \"test\"\n",
    "root_path = 'gdrive/My Drive/Colab Notebooks/Text-Classification'\n",
    "datasetPath = root_path + '/Disaster Tweets/Dataset/'\n",
    "resultsPath = root_path + '/Disaster Tweets/Results/'\n",
    "archPath = root_path + '/Disaster Tweets/Architecture/'\n",
    "glovePath = [root_path + '/Glove/glove.6B.50d.txt',\n",
    "             root_path + '/Glove/glove.6B.100d.txt',\n",
    "             root_path + '/Glove/glove.6B.200d.txt',\n",
    "             root_path + '/Glove/glove.6B.300d.txt']\n",
    "bestParameters = root_path + '/Disaster Tweets/BestResults.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMUrUZl-ZXh8"
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 271976,
     "status": "ok",
     "timestamp": 1586755879015,
     "user": {
      "displayName": "Tirana Fatyanosa",
      "photoUrl": "",
      "userId": "14754286705811282266"
     },
     "user_tz": -540
    },
    "id": "U8c9Yn-RXmdF",
    "outputId": "586aa56a-79d1-493f-b37c-3f844eb74748"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6872 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues1.h5\n",
      "5329/5329 [==============================] - 3s 656us/sample - loss: 0.6872 - f1_score: 0.0000e+00 - val_loss: 0.6826 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6833 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6820 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 115us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "5203/5329 [============================>.] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 117us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 115us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 114us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.255784034729004\n",
      "0.0\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6872 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues2.h5\n",
      "5329/5329 [==============================] - 2s 369us/sample - loss: 0.6873 - f1_score: 0.0000e+00 - val_loss: 0.6832 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6844 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6835 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6835 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.46034049987793\n",
      "0.0\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6870 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues3.h5\n",
      "5329/5329 [==============================] - 2s 307us/sample - loss: 0.6865 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 114us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.514837503433228\n",
      "0.0\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6860 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues4.h5\n",
      "5329/5329 [==============================] - 2s 315us/sample - loss: 0.6860 - f1_score: 0.0000e+00 - val_loss: 0.6823 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 115us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6828 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6840 - f1_score: 0.0000e+00 - val_loss: 0.6821 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 114us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 114us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6819 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6834 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6840 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6835 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6850 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.449924230575562\n",
      "0.0\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6865 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues5.h5\n",
      "5329/5329 [==============================] - 2s 318us/sample - loss: 0.6866 - f1_score: 0.0000e+00 - val_loss: 0.6823 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 114us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6821 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 117us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 124us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "5082/5329 [===========================>..] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 120us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "5082/5329 [===========================>..] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 121us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "5082/5329 [===========================>..] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 122us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "5082/5329 [===========================>..] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 121us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6846 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 122us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "5082/5329 [===========================>..] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 122us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.867269515991211\n",
      "0.0\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6862 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues6.h5\n",
      "5329/5329 [==============================] - 2s 320us/sample - loss: 0.6861 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5082/5329 [===========================>..] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 170us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "5082/5329 [===========================>..] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 119us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "5203/5329 [============================>.] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 122us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 114us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "5203/5329 [============================>.] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 116us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.801709413528442\n",
      "0.0\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6860 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues7.h5\n",
      "5329/5329 [==============================] - 2s 313us/sample - loss: 0.6859 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6846 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6835 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.377669334411621\n",
      "0.0\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_7 (Glob (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6859 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues8.h5\n",
      "5329/5329 [==============================] - 2s 304us/sample - loss: 0.6865 - f1_score: 0.0000e+00 - val_loss: 0.6823 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6819 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6831 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 104us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6821 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 104us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.055129766464233\n",
      "0.0\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_8 (Glob (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6861 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues9.h5\n",
      "5329/5329 [==============================] - 2s 348us/sample - loss: 0.6864 - f1_score: 0.0000e+00 - val_loss: 0.6821 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6846 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6834 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.293845415115356\n",
      "0.0\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_9 (Glob (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6865 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues10.h5\n",
      "5329/5329 [==============================] - 2s 323us/sample - loss: 0.6865 - f1_score: 0.0000e+00 - val_loss: 0.6821 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 104us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6850 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6846 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 104us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.114176988601685\n",
      "0.0\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_10 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6882 - f1_score: 0.0260\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues11.h5\n",
      "5329/5329 [==============================] - 2s 301us/sample - loss: 0.6882 - f1_score: 0.0237 - val_loss: 0.6829 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 104us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6834 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.150403499603271\n",
      "0.0\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_11 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6869 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues12.h5\n",
      "5329/5329 [==============================] - 2s 356us/sample - loss: 0.6867 - f1_score: 0.0000e+00 - val_loss: 0.6828 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6846 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6844 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 104us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6853 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6852 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.333689451217651\n",
      "0.0\n",
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_12 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6877 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues13.h5\n",
      "5329/5329 [==============================] - 2s 306us/sample - loss: 0.6873 - f1_score: 0.0000e+00 - val_loss: 0.6825 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6849 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6840 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6850 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6850 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6851 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.072779655456543\n",
      "0.0\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_13 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6855 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues14.h5\n",
      "5329/5329 [==============================] - 2s 308us/sample - loss: 0.6859 - f1_score: 0.0000e+00 - val_loss: 0.6825 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6844 - f1_score: 0.0000e+00 - val_loss: 0.6821 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6819 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6857 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.34747862815857\n",
      "0.0\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_14 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6870 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues15.h5\n",
      "5329/5329 [==============================] - 2s 306us/sample - loss: 0.6864 - f1_score: 0.0000e+00 - val_loss: 0.6821 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6850 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "5082/5329 [===========================>..] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 162us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6852 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6851 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6846 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6850 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6833 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6821 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.483211517333984\n",
      "0.0\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_15 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6884 - f1_score: 0.0621\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues16.h5\n",
      "5329/5329 [==============================] - 2s 308us/sample - loss: 0.6876 - f1_score: 0.0552 - val_loss: 0.6829 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6849 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6844 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6835 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6819 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.20884084701538\n",
      "0.0\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_16 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6865 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues17.h5\n",
      "5329/5329 [==============================] - 2s 306us/sample - loss: 0.6864 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6849 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6835 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6819 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6832 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.181052207946777\n",
      "0.0\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_17 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6885 - f1_score: 0.0275\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues18.h5\n",
      "5329/5329 [==============================] - 2s 309us/sample - loss: 0.6876 - f1_score: 0.0244 - val_loss: 0.6824 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6833 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6835 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6850 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.575582265853882\n",
      "0.0\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_18 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6866 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues19.h5\n",
      "5329/5329 [==============================] - 2s 310us/sample - loss: 0.6861 - f1_score: 0.0000e+00 - val_loss: 0.6821 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6821 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6833 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.29341197013855\n",
      "0.0\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_19 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6856 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues20.h5\n",
      "5329/5329 [==============================] - 2s 317us/sample - loss: 0.6851 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6856 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6830 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6850 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.286378622055054\n",
      "0.0\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_20 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_20 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6865 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues21.h5\n",
      "5329/5329 [==============================] - 2s 327us/sample - loss: 0.6866 - f1_score: 0.0000e+00 - val_loss: 0.6825 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 115us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6819 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.440114259719849\n",
      "0.0\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_21 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_21 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6863 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues22.h5\n",
      "5329/5329 [==============================] - 2s 307us/sample - loss: 0.6864 - f1_score: 0.0000e+00 - val_loss: 0.6830 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6846 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6844 - f1_score: 0.0000e+00 - val_loss: 0.6819 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6819 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 113us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "5203/5329 [============================>.] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 116us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.665122032165527\n",
      "0.0\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_22 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_22 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6874 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues23.h5\n",
      "5329/5329 [==============================] - 2s 315us/sample - loss: 0.6870 - f1_score: 0.0000e+00 - val_loss: 0.6821 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.30808401107788\n",
      "0.0\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_23 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_23 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6876 - f1_score: 0.0445\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues24.h5\n",
      "5329/5329 [==============================] - 2s 314us/sample - loss: 0.6876 - f1_score: 0.0406 - val_loss: 0.6824 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6834 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.312165975570679\n",
      "0.0\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_24 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_24 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6871 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues25.h5\n",
      "5329/5329 [==============================] - 2s 313us/sample - loss: 0.6870 - f1_score: 0.0000e+00 - val_loss: 0.6823 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6820 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6828 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6840 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 112us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 111us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6833 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.278854608535767\n",
      "0.0\n",
      "Model: \"sequential_25\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_25 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_25 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6875 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues26.h5\n",
      "5329/5329 [==============================] - 2s 307us/sample - loss: 0.6870 - f1_score: 0.0000e+00 - val_loss: 0.6828 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6834 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6847 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.483381271362305\n",
      "0.0\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_26 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_26 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6868 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues27.h5\n",
      "5329/5329 [==============================] - 2s 310us/sample - loss: 0.6862 - f1_score: 0.0000e+00 - val_loss: 0.6819 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6846 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6842 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6841 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6833 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6819 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.154219388961792\n",
      "0.0\n",
      "Model: \"sequential_27\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_27 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_27 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6871 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues28.h5\n",
      "5329/5329 [==============================] - 2s 305us/sample - loss: 0.6866 - f1_score: 0.0000e+00 - val_loss: 0.6817 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6839 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6819 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "5324/5329 [============================>.] - ETA: 0s - loss: 0.6840 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6840 - f1_score: 0.0000e+00 - val_loss: 0.6822 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6843 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6830 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.115710020065308\n",
      "0.0\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_28 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_28 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6875 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues29.h5\n",
      "5329/5329 [==============================] - 2s 309us/sample - loss: 0.6877 - f1_score: 0.0000e+00 - val_loss: 0.6825 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 109us/sample - loss: 0.6844 - f1_score: 0.0000e+00 - val_loss: 0.6816 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6853 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6838 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6850 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6837 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6844 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6843 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.12661600112915\n",
      "0.0\n",
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 73, 100)           1360200   \n",
      "_________________________________________________________________\n",
      "conv1d_29 (Conv1D)           (None, 376, 98)           82720     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_29 (Glo (None, 98)                0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 12)                1188      \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 12)                156       \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 12)                0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 1,444,277\n",
      "Trainable params: 1,444,277\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 5329 samples, validate on 2284 samples\n",
      "Epoch 1/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6867 - f1_score: 0.0000e+00\n",
      "Epoch 00001: val_f1_score improved from -inf to 0.00000, saving model to gdrive/My Drive/Colab Notebooks/Text-Classification/Disaster Tweets/Architecture/RandomSearch_8_SameValues30.h5\n",
      "5329/5329 [==============================] - 2s 306us/sample - loss: 0.6869 - f1_score: 0.0000e+00 - val_loss: 0.6831 - val_f1_score: 0.0000e+00\n",
      "Epoch 2/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6846 - f1_score: 0.0000e+00\n",
      "Epoch 00002: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6844 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 3/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6835 - f1_score: 0.0000e+00\n",
      "Epoch 00003: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 105us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6812 - val_f1_score: 0.0000e+00\n",
      "Epoch 4/10\n",
      "4840/5329 [==========================>...] - ETA: 0s - loss: 0.6836 - f1_score: 0.0000e+00\n",
      "Epoch 00004: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 110us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 5/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6846 - f1_score: 0.0000e+00\n",
      "Epoch 00005: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 108us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 6/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6845 - f1_score: 0.0000e+00\n",
      "Epoch 00006: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6813 - val_f1_score: 0.0000e+00\n",
      "Epoch 7/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6851 - f1_score: 0.0000e+00\n",
      "Epoch 00007: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6842 - f1_score: 0.0000e+00 - val_loss: 0.6815 - val_f1_score: 0.0000e+00\n",
      "Epoch 8/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6834 - f1_score: 0.0000e+00\n",
      "Epoch 00008: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6814 - val_f1_score: 0.0000e+00\n",
      "Epoch 9/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6848 - f1_score: 0.0000e+00\n",
      "Epoch 00009: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 106us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6818 - val_f1_score: 0.0000e+00\n",
      "Epoch 10/10\n",
      "4961/5329 [==========================>...] - ETA: 0s - loss: 0.6835 - f1_score: 0.0000e+00\n",
      "Epoch 00010: val_f1_score did not improve from 0.00000\n",
      "5329/5329 [==============================] - 1s 107us/sample - loss: 0.6841 - f1_score: 0.0000e+00 - val_loss: 0.6822 - val_f1_score: 0.0000e+00\n",
      "8.512304306030273\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import csv\n",
    "import tensorflow as tf\n",
    "\n",
    "# parameters\n",
    "n_run = 30\n",
    "\n",
    "# object class\n",
    "util = utility()\n",
    "cnn = CNN()\n",
    "\n",
    "with open(bestParameters, mode='r') as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    hyperparameters = {}\n",
    "    num_columns = len(next(reader))\n",
    "    infile.seek(0)\n",
    "    for row in reader:\n",
    "        if row[0] == 'layers':\n",
    "            hyperparameters[row[0]] = [row[column] for column in range(1, num_columns)]\n",
    "        else:\n",
    "            if 'activation' in row[0] or 'optimizer' in row[0] or 'init' in row[0]:\n",
    "                hyperparameters[row[0]] = row[1]\n",
    "            elif 'rate' in row[0] or 'momentum' in row[0]:\n",
    "                hyperparameters[row[0]] = float(row[1])\n",
    "            else:\n",
    "                hyperparameters[row[0]] = int(row[1])\n",
    "\n",
    "# Read data\n",
    "df = util.read_CSV(datasetPath + dataTraining)\n",
    "dfTesting = util.read_CSV(datasetPath + dataTesting)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "dfTraining, dfVal = train_test_split(df, test_size = 0.3, random_state=42)\n",
    "\n",
    "# get texts and labels\n",
    "textsTraining, y_train = util.get_text_label(dfTraining)\n",
    "textsVal, y_val = util.get_text_label(dfVal)\n",
    "textsTesting, y_test = util.get_text_label(dfTesting)\n",
    "\n",
    "# Tokenize words\n",
    "tokenizer = util.tokenize_texts(textsTraining)\n",
    "X_train = tokenizer.texts_to_sequences(textsTraining)\n",
    "X_val = tokenizer.texts_to_sequences(textsVal)\n",
    "X_test = tokenizer.texts_to_sequences(textsTesting)\n",
    "\n",
    "# Adding 1 because of reserved 0 index\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# get maxlen\n",
    "maxlen = util.get_max_length_of_sentences(textsTraining)\n",
    "\n",
    "# Pad sequences with zeros\n",
    "X_train = util.padding_texts(X_train, maxlen)\n",
    "X_val = util.padding_texts(X_val, maxlen)\n",
    "X_test = util.padding_texts(X_test, maxlen)\n",
    "\n",
    "if int(hyperparameters['output_dim']) == 50:\n",
    "    glove = glovePath[0]\n",
    "elif int(hyperparameters['output_dim']) == 100:\n",
    "    glove = glovePath[1]\n",
    "elif int(hyperparameters['output_dim']) == 200:\n",
    "    glove = glovePath[2]\n",
    "elif int(hyperparameters['output_dim']) == 300:\n",
    "    glove = glovePath[3]\n",
    "\n",
    "embedding_matrix = util.create_embedding_matrix(glove, tokenizer.word_index, int(hyperparameters['output_dim']))\n",
    "\n",
    "# Create Testing Results\n",
    "f = open(resultsPath + testing_name + \".csv\", \"w+\")\n",
    "f.write(\"i,accuracy,precision,recall,f1Score,time\\n\")\n",
    "f.close()\n",
    "\n",
    "for i in range(0, n_run):\n",
    "    then = time.time()\n",
    "    # grid search\n",
    "    model = cnn.cnn_model(vocab_size, maxlen, embedding_matrix, hyperparameters, hyperparameters['layers'])\n",
    "    \n",
    "    # save history to a file\n",
    "    callbacks = [tf.keras.callbacks.CSVLogger(str(archPath + testing_name + str(i + 1) + \".csv\"))]\n",
    "\n",
    "    #early stopping\n",
    "    callbacks += [tf.keras.callbacks.EarlyStopping(monitor='val_f1_score', mode='max', verbose=1, patience=10)]\n",
    "\n",
    "    #save the best model\n",
    "    callbacks += [tf.keras.callbacks.ModelCheckpoint(archPath + testing_name + str(i + 1) + \".h5\", monitor='val_f1_score', mode='max', verbose=1, save_best_only=True)]\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    y_train = np.uint8(y_train)\n",
    "    y_val = np.uint8(y_val)\n",
    "    y_test = np.uint8(y_test)\n",
    "    model.fit(X_train, y_train, epochs=hyperparameters['epochs'], verbose=True, validation_data=(X_val, y_val),\n",
    "              batch_size=hyperparameters['batch_size'], callbacks=callbacks)\n",
    "    \n",
    "    # save model to a file\n",
    "    # model.save(archPath + testing_name + str(i + 1) + \".h5\")\n",
    "\n",
    "    dependencies = {\n",
    "    'f1_score': util.f1_score\n",
    "    }\n",
    "\n",
    "    # load the saved model\n",
    "    saved_model = tf.keras.models.load_model(archPath + testing_name + str(i + 1) + \".h5\", custom_objects=dependencies)   \n",
    "\n",
    "    y_pred = saved_model.predict_classes(X_test)\n",
    "    dfTesting['target'] = y_pred\n",
    "    util.write_df_csv(dfTesting, resultsPath + testing_name + str(i + 1) + \".csv\")\n",
    "    \n",
    "\n",
    "    # CNN metrics\n",
    "    accuracyScore, precisionScore, recallScore, f1Score = util.get_testing_metric(y_test, y_pred)\n",
    "\n",
    "    now = time.time()\n",
    "    diff = now - then\n",
    "    print(diff)\n",
    "    print(f1Score)\n",
    "\n",
    "    # save testing data\n",
    "    f = open(\"{}{}.csv\".format(resultsPath, testing_name), 'a')\n",
    "    text = \"{0},{1},{2},{3},{4},{5}\\n\".format(str(i + 1),str(accuracyScore), str(precisionScore), str(recallScore), str(f1Score), str(diff))\n",
    "    f.write(text)\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CNNSameValues.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
